{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "35dd57d5-2b79-49d8-88df-cc272b63832f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset to inspect it\n",
    "file_path = 'Titanic-Dataset.csv'\n",
    "titanic_data = pd.read_csv(file_path)\n",
    "\n",
    "# Display the first few rows of the dataset to understand its structure\n",
    "titanic_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b306866b-5b54-4e40-8474-61cf6fa3a58f",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (82521612.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[13], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    '''\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "The dataset contains the following columns:\n",
    "\n",
    "PassengerId: Unique identifier for each passenger.\n",
    "Survived: Binary outcome (0 = Did not survive, 1 = Survived).\n",
    "Pclass: Passenger class (1 = 1st, 2 = 2nd, 3 = 3rd).\n",
    "Name: Passenger name.\n",
    "Sex: Gender of the passenger.\n",
    "Age: Age of the passenger.\n",
    "SibSp: Number of siblings/spouses aboard.\n",
    "Parch: Number of parents/children aboard.\n",
    "Ticket: Ticket number.\n",
    "Fare: Passenger fare.\n",
    "Cabin: Cabin number (many missing values).\n",
    "Embarked: Port of embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22036dd4-c055-47a2-b6cd-6b3354b8b0c8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (333038693.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[14], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    '''\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "To proceed with building the Titanic survival prediction model, we will perform the following steps:\n",
    "\n",
    "Handle missing values.\n",
    "Perform feature engineering (e.g., encoding categorical features).\n",
    "Split the data into training and testing sets.\n",
    "Build and train the machine learning model.\n",
    "Evaluate the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8bb67408-773a-4b6a-a9d8-649ee5d8b396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PassengerId      0\n",
       "Survived         0\n",
       "Pclass           0\n",
       "Name             0\n",
       "Sex              0\n",
       "Age            177\n",
       "SibSp            0\n",
       "Parch            0\n",
       "Ticket           0\n",
       "Fare             0\n",
       "Cabin          687\n",
       "Embarked         2\n",
       "dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for missing values in the dataset\n",
    "titanic_data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf087d4-9819-4cb0-87c3-8bd4143478fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "The dataset contains missing values in the following columns:\n",
    "\n",
    "Age: 177 missing values.\n",
    "Cabin: 687 missing values (many entries are missing, we may drop this column).\n",
    "Embarked: 2 missing values.\n",
    "We will:\n",
    "\n",
    "Impute missing values in the Age column using the median.\n",
    "Drop the Cabin column due to a high number of missing values.\n",
    "Impute missing values in the Embarked column with the most frequent value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "c55e1688-6588-4297-a073-bbc2b3b30c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\umesh\\AppData\\Local\\Temp\\ipykernel_4700\\23307280.py:2: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  titanic_data['Age'].fillna(titanic_data['Age'].median(), inplace=True)\n",
      "C:\\Users\\umesh\\AppData\\Local\\Temp\\ipykernel_4700\\23307280.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "PassengerId    0\n",
       "Survived       0\n",
       "Pclass         0\n",
       "Name           0\n",
       "Sex            0\n",
       "Age            0\n",
       "SibSp          0\n",
       "Parch          0\n",
       "Ticket         0\n",
       "Fare           0\n",
       "Embarked       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impute missing Age values with the median\n",
    "titanic_data['Age'].fillna(titanic_data['Age'].median(), inplace=True)\n",
    "\n",
    "# Drop the Cabin column\n",
    "titanic_data.drop(columns=['Cabin'], inplace=True)\n",
    "\n",
    "# Impute missing Embarked values with the most frequent value\n",
    "titanic_data['Embarked'].fillna(titanic_data['Embarked'].mode()[0], inplace=True)\n",
    "\n",
    "# Verify if missing values have been handled\n",
    "titanic_data.isnull().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103a69d5-47c7-4008-ac3a-d40ec87fea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "All missing values have been successfully handled. Now, we will proceed with feature engineering:\n",
    "\n",
    "Convert categorical variables:\n",
    "Encode Sex (male/female) into binary values.\n",
    "Encode Embarked (C, Q, S) into numerical values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0fd8527f-f7a3-4d1a-a6a4-75ab78a8ec0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-hot encoding for 'Embarked' applied.\n"
     ]
    }
   ],
   "source": [
    "#Handle the Embarked Column\n",
    "#If the Embarked column still exists, you can one-hot encode it as follows. If not, you can skip this step and proceed with the rest of the data cleaning.\n",
    "# Check if 'Embarked' is in the dataset\n",
    "if 'Embarked' in titanic_data.columns:\n",
    "    # One-hot encode 'Embarked'\n",
    "    titanic_data = pd.get_dummies(titanic_data, columns=['Embarked'], drop_first=True)\n",
    "    print(\"One-hot encoding for 'Embarked' applied.\")\n",
    "else:\n",
    "    print(\"'Embarked' column not found. Skipping this step.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ce10b11c-ce1e-41fd-ab9f-942177ab0ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 'Sex' into binary (0 for male, 1 for female)\n",
    "titanic_data['Sex'] = titanic_data['Sex'].map({'male': 0, 'female': 1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a12248f5-2078-427a-8b41-10a228cef74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop Unnecessary Columns\n",
    "#Now, let's drop columns that are not useful for prediction, such as PassengerId, Name, and Ticket.\n",
    "# Drop irrelevant columns\n",
    "titanic_data.drop(columns=['PassengerId', 'Name', 'Ticket'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "9631511c-93a6-4d7a-86c5-976fd3340c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split the Dataset for Training and Testing\n",
    "#We will now split the cleaned dataset into training and testing sets.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define features (X) and target (y)\n",
    "X = titanic_data.drop('Survived', axis=1)\n",
    "y = titanic_data['Survived']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "09b1eb34-5d5c-43f2-b887-40a3868cdadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 81.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\umesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#Build a Logistic Regression Model\n",
    "#Once the data is prepared, we can build a machine learning model, starting with Logistic Regression.\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize the logistic regression model\n",
    "model = LogisticRegression()\n",
    "\n",
    "# Train the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b6cc22a8-3e63-4cfd-80b2-3c045c4a022a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.86      0.84       105\n",
      "           1       0.79      0.74      0.76        74\n",
      "\n",
      "    accuracy                           0.81       179\n",
      "   macro avg       0.81      0.80      0.80       179\n",
      "weighted avg       0.81      0.81      0.81       179\n",
      "\n",
      "Confusion Matrix:\n",
      "[[90 15]\n",
      " [19 55]]\n"
     ]
    }
   ],
   "source": [
    "#Evaluate the Model\n",
    "#After training the logistic regression model, \n",
    "#we can further evaluate its performance using various metrics such as \n",
    "#classification report and confusion matrix.\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Print classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Print confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402a0aea-2949-4df2-b621-75535f23b6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Improving the Model \n",
    "#If the modelâ€™s performance is not satisfactory, you could:\n",
    "'''\n",
    "Try other algorithms (e.g., Decision Trees, Random Forest, etc.).\n",
    "Tune hyperparameters.\n",
    "Add more features (e.g., interactions between features) or engineer new ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81b21280-b4fb-423f-9ddd-9d76eef1b3d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Accuracy: 79.89%\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize the model\n",
    "rf_model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "rf_y_pred = rf_model.predict(X_test)\n",
    "\n",
    "# Evaluate the Random Forest model\n",
    "rf_accuracy = accuracy_score(y_test, rf_y_pred)\n",
    "print(f'Random Forest Accuracy: {rf_accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "589d8504-bd78-46de-a156-0e6a497f78c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Validation Accuracy: 78.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\umesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\umesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\umesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\umesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\Users\\umesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "#We can also use cross-validation for more robust performance evaluation:\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Perform cross-validation\n",
    "cv_scores = cross_val_score(model, X, y, cv=5)\n",
    "print(f\"Cross-Validation Accuracy: {cv_scores.mean() * 100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c171a4c8-3a45-4d13-a2e4-e90fee65018c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "#Feature engineering involves creating new features that may help the model better capture patterns in the data. Here are a few ideas:\n",
    "\n",
    "#Family Size: Combine SibSp (siblings/spouses aboard) and Parch (parents/children aboard) to create a Family Size feature.\n",
    "# Create a new feature for Family Size\n",
    "titanic_data['FamilySize'] = titanic_data['SibSp'] + titanic_data['Parch'] + 1  # Add 1 for the passenger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ed715674-e210-4687-a251-5b3e3c81fc2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\umesh\\AppData\\Local\\Temp\\ipykernel_4700\\1761040356.py:4: FutureWarning: ChainedAssignmentError: behaviour will change in pandas 3.0!\n",
      "You are setting values through chained assignment. Currently this works in certain cases, but when using Copy-on-Write (which will become the default behaviour in pandas 3.0) this will never work to update the original DataFrame or Series, because the intermediate object on which we are setting values will behave as a copy.\n",
      "A typical example is when you are setting values in a column of a DataFrame, like:\n",
      "\n",
      "df[\"col\"][row_indexer] = value\n",
      "\n",
      "Use `df.loc[row_indexer, \"col\"] = values` instead, to perform the assignment in a single step and ensure this keeps updating the original `df`.\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "\n",
      "  titanic_data['IsAlone'].loc[titanic_data['FamilySize'] > 1] = 0  # Set to 0 if FamilySize > 1\n",
      "C:\\Users\\umesh\\AppData\\Local\\Temp\\ipykernel_4700\\1761040356.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  titanic_data['IsAlone'].loc[titanic_data['FamilySize'] > 1] = 0  # Set to 0 if FamilySize > 1\n"
     ]
    }
   ],
   "source": [
    "#IsAlone: A binary feature indicating whether the passenger was traveling alone.\n",
    "# Create a feature to indicate if the passenger was alone\n",
    "titanic_data['IsAlone'] = 1  # Initialize to 1 (True)\n",
    "titanic_data['IsAlone'].loc[titanic_data['FamilySize'] > 1] = 0  # Set to 0 if FamilySize > 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1561c9a7-c8d0-4cb0-923b-93feb9622b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fare Binning: Group the Fare feature into categories for better interpretation by the model.\n",
    "titanic_data['FareBin'] = pd.qcut(titanic_data['Fare'], 4, labels=[1, 2, 3, 4])  # Quartile binning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "e8ac504a-2889-4be4-b705-e83c6a058a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Age Binning: Similarly, you can group Age into age ranges\n",
    "# Create age categories\n",
    "titanic_data['AgeBin'] = pd.cut(titanic_data['Age'], bins=[0, 12, 20, 40, 60, 80], labels=[1, 2, 3, 4, 5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "72505758-2172-473d-9a63-c2e47670cc55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#After creating these new features,We will drop the original columns (if applicable) to avoid multicollinearity.\n",
    "# Drop the original columns after feature engineering\n",
    "titanic_data.drop(columns=['SibSp', 'Parch', 'Age', 'Fare'], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e99bd107-f3f1-4afe-834a-689c5793ba42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\umesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 81.01%\n",
      "Random Forest Accuracy: 79.89%\n",
      "Gradient Boosting Accuracy: 80.45%\n",
      "Support Vector Machine Accuracy: 65.36%\n",
      "K-Nearest Neighbors Accuracy: 71.51%\n"
     ]
    }
   ],
   "source": [
    "#Model Comparison\n",
    "#Once We have engineered new features, we can compare different machine learning models.\n",
    "#Logistic Regression (already implemented).\n",
    "#Random Forest Classifier.\n",
    "#Gradient Boosting Classifier.\n",
    "#Support Vector Machines (SVM).\n",
    "#K-Nearest Neighbors (KNN).\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42),\n",
    "    'Support Vector Machine': SVC(),\n",
    "    'K-Nearest Neighbors': KNeighborsClassifier()\n",
    "}\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f'{model_name} Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8eda9328-12eb-43f4-9417-aa59fe855baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\umesh\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Model Accuracy: 81.01%\n"
     ]
    }
   ],
   "source": [
    "#Ensemble Methods\n",
    "#We can also combine multiple models to improve prediction accuracy using ensemble techniques like Voting Classifier:\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "# Combine different models in a voting classifier\n",
    "voting_clf = VotingClassifier(estimators=[\n",
    "    ('lr', LogisticRegression()),\n",
    "    ('rf', RandomForestClassifier(n_estimators=100)),\n",
    "    ('gb', GradientBoostingClassifier(n_estimators=100))\n",
    "], voting='hard')\n",
    "\n",
    "# Train the ensemble model\n",
    "voting_clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the ensemble model\n",
    "y_pred = voting_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Ensemble Model Accuracy: {accuracy * 100:.2f}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
